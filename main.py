import streamlit as st
import requests
import re
from urllib.parse import urlparse
import json
from bs4 import BeautifulSoup
import openai
import unicodedata
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# –°–µ–∫—Ä–µ—Ç–∏
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
GSHEET_JSON = st.secrets["GSHEET_SERVICE_ACCOUNT"]
GSHEET_SPREADSHEET_ID = "1S0nkJYXrVTsMHmeOC-uvMWnrw_yQi5z8NzRsJEcBjc0"

# –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Google Sheets
def get_gsheet_client():
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    creds_dict = json.loads(GSHEET_JSON)
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    return gspread.authorize(creds)

# –°–ø—Ä–æ—â–µ–Ω–Ω—è URL
def simplify_url(link):
    parsed = urlparse(link)
    return f"{parsed.scheme}://{parsed.netloc}"

# –Ü–Ω—Ç–µ—Ä—Ñ–µ–π—Å
st.set_page_config(page_title="–ü–æ—à—É–∫ —Å–∞–π—Ç—ñ–≤", layout="wide")
st.title("üîç –ü–æ—à—É–∫ —Å–∞–π—Ç—ñ–≤ –±–µ–∑ –∞–Ω–∞–ª—ñ–∑—É (—Ç—ñ–ª—å–∫–∏ –≤–∫–ª–∞–¥–∫–∞ '–ü–æ—à—É–∫–∏')")

query = st.text_input("–í–≤–µ–¥–∏ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞:")
col1, col2 = st.columns(2)
with col1:
    num_results = st.slider("–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤", min_value=1, max_value=100, value=10, step=1)
with col2:
    start_options = list(range(1, 101, 10))
    start_index = st.selectbox("–°—Ç–æ—Ä—ñ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤:", options=start_options, index=0)

start = st.button("–ü–æ—à—É–∫")

if start and query:
    st.markdown("üîÅ **–¢—Ä–∏–≥–µ—Ä –∞–∫—Ç–∏–≤–æ–≤–∞–Ω–æ ‚Äî –≤–∏–∫–æ–Ω—É—î—Ç—å—Å—è –ø–æ—à—É–∫...**")  # DEBUG

    with st.spinner("–ü–æ—à—É–∫ —Å–∞–π—Ç—ñ–≤..."):
        params = {
            "key": st.secrets["GOOGLE_API_KEY"],
            "cx": st.secrets["CSE_ID"],
            "q": query,
            "num": num_results,
            "start": start_index
        }
        results = requests.get("https://www.googleapis.com/customsearch/v1", params=params).json().get("items", [])

        page_number = ((start_index - 1) // num_results) + 1

        st.markdown(f"### üîç –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è: **{query}**")
        st.markdown(f"‚û°Ô∏è **–°—Ç–æ—Ä—ñ–Ω–∫–∞ ‚Ññ{page_number}** (start_index = `{start_index}`, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –æ—Ç—Ä–∏–º–∞–Ω–æ: `{len(results)}`)")

        gc = get_gsheet_client()
        sh = gc.open_by_key(GSHEET_SPREADSHEET_ID)

        try:
            search_sheet = sh.worksheet("–ü–æ—à—É–∫–∏")
        except:
            search_sheet = sh.add_worksheet(title="–ü–æ—à—É–∫–∏", rows="1000", cols="5")
            search_sheet.append_row(["–ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞", "–ù–∞–∑–≤–∞", "–°–∞–π—Ç", "–°—Ç–æ—Ä—ñ–Ω–∫–∞", "–î–∞—Ç–∞"], value_input_option="USER_ENTERED")

        existing_links = set(search_sheet.col_values(3))
        new_count = 0

        for item in results:
            title = item.get("title", "")
            raw_link = item.get("link", "")
            simplified = simplify_url(raw_link)

            if simplified in existing_links:
                st.markdown(f"üîÅ –ü—Ä–æ–ø—É—â–µ–Ω–æ (–≤–∂–µ —î): `{simplified}`")
                continue

            st.markdown(f"‚úÖ –î–æ–¥–∞–Ω–æ: **{title}** ‚Äî `{simplified}`")

            search_sheet.append_row(
                [query, title, simplified, page_number, st.session_state.get("current_date", "")],
                value_input_option="USER_ENTERED"
            )
            existing_links.add(simplified)
            new_count += 1

        st.success(f"üü¢ –î–æ–¥–∞–Ω–æ {new_count} –Ω–æ–≤–∏—Ö —Å–∞–π—Ç—ñ–≤ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {page_number}")



        
        # --------------------- GPT-–ê–Ω–∞–ª—ñ–∑ –Ω–æ–≤–∏—Ö —Å–∞–π—Ç—ñ–≤ ---------------------
client = openai.OpenAI(api_key=OPENAI_API_KEY)

st.header("ü§ñ GPT-–ê–Ω–∞–ª—ñ–∑ –Ω–æ–≤–∏—Ö —Å–∞–π—Ç—ñ–≤")

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É —Ç–µ–∫—Å—Ç—É —Å–∞–π—Ç—É
def get_page_text(url):
    try:
        response = requests.get(url, timeout=7)
        soup = BeautifulSoup(response.text, "html.parser")
        for tag in soup(["script", "style", "noscript"]):
            tag.extract()
        text = soup.get_text(separator=" ")
        return ' '.join(text.split())[:2000]  # –æ–±–º–µ–∂–µ–Ω–Ω—è –¥–æ 2000 —Å–∏–º–≤–æ–ª—ñ–≤
    except Exception as e:
        return f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ —Ç–µ–∫—Å—Ç —Å–∞–π—Ç—É: {e}"

# –°–∫—ñ–ª—å–∫–∏ —Å–∞–π—Ç—ñ–≤ –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∑–∞ —Ä–∞–∑
num_to_analyze = st.slider("–°–∫—ñ–ª—å–∫–∏ –∑–∞–ø–∏—Å—ñ–≤ –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∑–∞ —Ä–∞–∑", min_value=1, max_value=50, value=10)

if st.button("–ê–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ –∑–∞–ø–∏—Å–∏ GPT"):
    with st.spinner("–ü—Ä–æ–≤–æ–¥–∏—Ç—å—Å—è GPT-–∞–Ω–∞–ª—ñ–∑..."):
        gc = get_gsheet_client()
        sh = gc.open_by_key(GSHEET_SPREADSHEET_ID)

        try:
            search_sheet = sh.worksheet("–ü–æ—à—É–∫–∏")
        except:
            st.error("–í–∫–ª–∞–¥–∫–∞ '–ü–æ—à—É–∫–∏' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞.")
            st.stop()

        try:
            analysis_sheet = sh.worksheet("–ê–Ω–∞–ª—ñ–∑")
        except:
            analysis_sheet = sh.add_worksheet(title="–ê–Ω–∞–ª—ñ–∑", rows="1000", cols="8")
            analysis_sheet.append_row(
                ["–ù–∞–∑–≤–∞", "–°–∞–π—Ç", "–ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞", "–í–∏—Å–Ω–æ–≤–æ–∫", "–ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π –∫–ª—ñ—î–Ω—Ç", "–°—Ç–æ—Ä—ñ–Ω–∫–∞", "–î–∞—Ç–∞", "–°—Ç–∞—Ç—É—Å GPT"],
                value_input_option="USER_ENTERED"
            )

        records = search_sheet.get_all_records()
        analyzed_sites = set()
        existing_analysis = analysis_sheet.get_all_records()
        for r in existing_analysis:
            site_val = r.get("–°–∞–π—Ç", "").strip().lower()
            if site_val:
                analyzed_sites.add(site_val)

        analyzed_count = 0
        for idx, row in enumerate(records, start=2):
            if analyzed_count >= num_to_analyze:
                break

            title = row.get("–ù–∞–∑–≤–∞", "")
            site = simplify_url(row.get("–°–∞–π—Ç", "").strip().lower())
            keywords = row.get("–ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞", "")
            page = row.get("–°—Ç–æ—Ä—ñ–Ω–∫–∞", "")
            date = row.get("–î–∞—Ç–∞", "")

            if site in analyzed_sites:
                continue

            try:
                site_text = get_page_text(site)

                prompt = f"""
                –¢–∏ ‚Äî –∞—Å–∏—Å—Ç–µ–Ω—Ç –∑ –ø—Ä–æ–¥–∞–∂—É –∫–æ–º–ø–∞–Ω—ñ—ó, —è–∫–∞ –ø–æ—Å—Ç–∞—á–∞—î —Ä–µ–Ω—Ç–≥–µ–Ω-–ø–ª—ñ–≤–∫—É, –∫–∞—Å–µ—Ç–∏, –º–µ–¥–∏—á–Ω—ñ –ø—Ä–∏–Ω—Ç–µ—Ä–∏ —Ç–∞ –≤–∏—Ç—Ä–∞—Ç–Ω—ñ –º–∞—Ç–µ—Ä—ñ–∞–ª–∏.

                –ù–∞–∑–≤–∞ –∫–æ–º–ø–∞–Ω—ñ—ó: {title}
                –°–∞–π—Ç: {site}
                –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞: {keywords}
                –ö–æ–Ω—Ç–µ–Ω—Ç —Å–∞–π—Ç—É (–æ–±–º–µ–∂–µ–Ω–æ): {site_text}

                –ó–∞–≤–¥–∞–Ω–Ω—è:
                - –í–∏–∑–Ω–∞—á–∏, —á–∏ –∫–æ–º–ø–∞–Ω—ñ—è —î –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–º –∫–ª—ñ—î–Ω—Ç–æ–º (–¢–∞–∫/–ù—ñ).
                - –Ø–∫—â–æ –¢–∞–∫, –≤–∫–∞–∂–∏ —ó—ó —Ç–∏–ø (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥: –¥–∏—Å—Ç—Ä–∏–±‚Äô—é—Ç–æ—Ä, –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫).
                - –î–∞–π –∫–æ—Ä–æ—Ç–∫–∏–π –≤–∏—Å–Ω–æ–≤–æ–∫ ‚Äî –æ–¥–Ω–µ —Ä–µ—á–µ–Ω–Ω—è, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥: "–¢–∞–∫, –¥–∏—Å—Ç—Ä–∏–±‚Äô—é—Ç–æ—Ä, –ø—Ä–∞—Ü—é—î –∑ –≤–∞—à–∏–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏."

                –§–æ—Ä–º–∞—Ç:
                –ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π –∫–ª—ñ—î–Ω—Ç: –¢–∞–∫/–ù—ñ
                –í–∏—Å–Ω–æ–≤–æ–∫: (–æ–¥–Ω–µ —Ä–µ—á–µ–Ω–Ω—è)
                """

                response = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": prompt}]
                )

                content = response.choices[0].message.content.strip()

                try:
                    client_match = re.search(r"–ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π –∫–ª—ñ—î–Ω—Ç:\s*(–¢–∞–∫|–ù—ñ)", content)
                    summary_match = re.search(r"–í–∏—Å–Ω–æ–≤–æ–∫:\s*(.+)", content)

                    is_client = client_match.group(1) if client_match else "-"
                    summary = summary_match.group(1).strip() if summary_match else content
                except Exception as parse_error:
                    is_client = "-"
                    summary = f"GPT format error: {parse_error}"

                status = "–ê–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ"

            except Exception as e:
                is_client = "-"
                summary = f"–ü–æ–º–∏–ª–∫–∞: {e}"
                status = "–ü–æ–º–∏–ª–∫–∞"

            # –î–æ–¥–∞—î–º–æ –¥–æ "–ê–Ω–∞–ª—ñ–∑" —á—ñ—Ç–∫–æ –≤ 8 –∫–æ–ª–æ–Ω–æ–∫
            analysis_row = [
                title or "-",     # –ù–∞–∑–≤–∞
                site or "-",      # –°–∞–π—Ç
                keywords or "-",  # –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞
                summary or "-",   # –í–∏—Å–Ω–æ–≤–æ–∫
                is_client or "-", # –ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π –∫–ª—ñ—î–Ω—Ç
                page or "-",      # –°—Ç–æ—Ä—ñ–Ω–∫–∞
                date or "-",      # –î–∞—Ç–∞
                status or "-"     # –°—Ç–∞—Ç—É—Å GPT
            ]
            analysis_sheet.append_row(analysis_row, value_input_option="USER_ENTERED")
                        # –ü—ñ—Å–ª—è –∑–∞–ø–∏—Å—É –≤ "–ê–Ω–∞–ª—ñ–∑" –æ–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç—É—Å –≤ "–ü–æ—à—É–∫–∏"
            try:
                search_sheet.update_cell(idx, 7, status)
            except Exception as update_error:
                st.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ–Ω–æ–≤–∏—Ç–∏ —Å—Ç–∞—Ç—É—Å –¥–ª—è '{title}': {update_error}")


            analyzed_sites.add(site)
            analyzed_count += 1

        st.success(f"‚úÖ GPT-–∞–Ω–∞–ª—ñ–∑ –≤–∏–∫–æ–Ω–∞–Ω–æ –¥–ª—è {analyzed_count} –Ω–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤.")

# --------------------- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–∑–≤ –∫–æ–º–ø–∞–Ω—ñ–π –∑ –≤–∫–ª–∞–¥–∫–∏ ---------------------
st.header("üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–∑–≤ –∫–æ–º–ø–∞–Ω—ñ–π –∑ —ñ–Ω—à–æ—ó –≤–∫–ª–∞–¥–∫–∏")

source_tab = st.text_input("–í–≤–µ–¥–∏ –Ω–∞–∑–≤—É –≤–∫–ª–∞–¥–∫–∏ –∑ –∫–æ–º–ø–∞–Ω—ñ—è–º–∏:")
load_companies = st.button("–ó—á–∏—Ç–∞—Ç–∏ –∫–æ–º–ø–∞–Ω—ñ—ó —Ç–∞ –¥–æ–ø–æ–≤–Ω–∏—Ç–∏ –≤–∫–ª–∞–¥–∫—É '–∫–æ–º–ø–∞–Ω—ñ—ó'")

if load_companies and source_tab:
    try:
        gc = get_gsheet_client()
        sh = gc.open_by_key(GSHEET_SPREADSHEET_ID)
        ws = sh.worksheet(source_tab)
        data = ws.col_values(1)[1:]  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫

        # –û—Ç—Ä–∏–º—É—î–º–æ –∞–±–æ —Å—Ç–≤–æ—Ä—é—î–º–æ –≤–∫–ª–∞–¥–∫—É "–∫–æ–º–ø–∞–Ω—ñ—ó"
        try:
            company_sheet = sh.worksheet("–∫–æ–º–ø–∞–Ω—ñ—ó")
            existing = set(name.strip().upper() for name in company_sheet.col_values(1)[1:])
        except:
            company_sheet = sh.add_worksheet(title="–∫–æ–º–ø–∞–Ω—ñ—ó", rows="1000", cols="1")
            company_sheet.update("A1", [["–ö–æ–º–ø–∞–Ω—ñ—ó"]])
            existing = set()

        log_output = []
        new_entries = []

        for name in data:
            if not name:
                continue
            original = name
            name = name.strip().lower()
            for prefix in ["—Ñ–æ–ø", "—Ç–æ–≤", "–ø–ø"]:
                if name.startswith(prefix):
                    name = name[len(prefix):].strip()
            name = name.replace("¬´", "").replace("¬ª", "").replace("\"", "")
            name = ' '.join(name.split())
            if len(name) > 2:
                cleaned = name.upper()
                if cleaned in existing or cleaned in [x[0] for x in new_entries]:
                    log_output.append(f"üîÅ –ü—Ä–æ–ø—É—â–µ–Ω–æ –ø–æ–≤—Ç–æ—Ä: {original}")
                else:
                    new_entries.append([cleaned])
                    log_output.append(f"‚ûï –î–æ–¥–∞–Ω–æ: {cleaned}")

        if new_entries:
            next_row = len(existing) + 2  # +2 –±–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ + 1-based
            company_sheet.update(f"A{next_row}:A{next_row + len(new_entries) - 1}", new_entries)

        st.success(f"‚úÖ –î–æ–¥–∞–Ω–æ –Ω–æ–≤–∏—Ö –∫–æ–º–ø–∞–Ω—ñ–π: {len(new_entries)}")
        st.markdown("### üìã –ñ—É—Ä–Ω–∞–ª –æ–±—Ä–æ–±–∫–∏:")
        for msg in log_output:
            st.markdown(msg)

    except Exception as e:
        st.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
        # --------------------- –ü–æ—à—É–∫ —Å–∞–π—Ç—ñ–≤ –∑–∞ –Ω–∞–∑–≤–∞–º–∏ –∑ –≤–∫–ª–∞–¥–∫–∏ "–∫–æ–º–ø–∞–Ω—ñ—ó" ---------------------
st.header("üåê –ü–æ—à—É–∫ —Å–∞–π—Ç—ñ–≤ –∑–∞ –Ω–∞–∑–≤–∞–º–∏ –∫–æ–º–ø–∞–Ω—ñ–π")

start_search = st.button("üîç –ü–æ—á–∞—Ç–∏ –ø–æ—à—É–∫ —Å–∞–π—Ç—ñ–≤")

if start_search:
    try:
        gc = get_gsheet_client()
        sh = gc.open_by_key(GSHEET_SPREADSHEET_ID)

        # –û—Ç—Ä–∏–º—É—î–º–æ –Ω–∞–∑–≤–∏ –∑ –≤–∫–ª–∞–¥–∫–∏ "–∫–æ–º–ø–∞–Ω—ñ—ó"
        company_sheet = sh.worksheet("–∫–æ–º–ø–∞–Ω—ñ—ó")
        company_names = [c.strip().upper() for c in company_sheet.col_values(1)[1:] if c.strip()]

        # –û—Ç—Ä–∏–º—É—î–º–æ –≤–∂–µ –æ–ø—Ä–∞—Ü—å–æ–≤–∞–Ω—ñ –∑ –≤–∫–ª–∞–¥–∫–∏ "—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏"
        try:
            results_sheet = sh.worksheet("—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏")
            processed_names = set(name.strip().upper() for name in results_sheet.col_values(1)[1:] if name.strip())
        except:
            results_sheet = sh.add_worksheet(title="—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏", rows="1000", cols="5")
            results_sheet.append_row(["–ö–æ–º–ø–∞–Ω—ñ—è", "–°–∞–π—Ç", "–ù–∞–∑–≤–∞ –∑ Google", "–°—Ç–æ—Ä—ñ–Ω–∫–∞", "–î–∞—Ç–∞"], value_input_option="USER_ENTERED")
            processed_names = set()

        to_process = [name for name in company_names if name not in processed_names]

        st.markdown(f"üîé –ó–∞–ª–∏—à–∏–ª–æ—Å—å –¥–æ –æ–±—Ä–æ–±–∫–∏: **{len(to_process)}** –∫–æ–º–ø–∞–Ω—ñ–π")
        num_checked = 0

        for name in to_process:
            params = {
                "key": st.secrets["GOOGLE_API_KEY"],
                "cx": st.secrets["CSE_ID"],
                "q": name,
                "num": 10
            }

            try:
                resp = requests.get("https://www.googleapis.com/customsearch/v1", params=params)
                results = resp.json().get("items", [])

                found = False

                for item in results:
                    title = item.get("title", "")
                    snippet = item.get("snippet", "")
                    link = item.get("link", "")

                    def clean_text(text):
                        import unicodedata
                        text = unicodedata.normalize('NFKD', text)
                        text = re.sub(r"[^\w\s]", "", text.lower())
                        return set(text.split())

                    name_words = clean_text(name)
                    combined_words = clean_text(title + " " + snippet)

                    if len(name_words.intersection(combined_words)) >= 2:
                        simplified = simplify_url(link)
                        today = pd.Timestamp.now().strftime("%Y-%m-%d")
                        results_sheet.append_row([name, simplified, title, "1", today], value_input_option="USER_ENTERED")
                        st.markdown(f"‚úÖ **{name}** ‚Üí `{simplified}`")
                        found = True
                        break

                if not found:
                    st.markdown(f"‚ö†Ô∏è **{name}** ‚Äî —Å–∞–π—Ç –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")

                num_checked += 1
                if num_checked >= 20:
                    st.info("‚è∏Ô∏è –û–±–º–µ–∂–µ–Ω–Ω—è: –æ–±—Ä–æ–±–ª–µ–Ω–æ 20 –∫–æ–º–ø–∞–Ω—ñ–π –∑–∞ —Ä–∞–∑. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç—ñ—Ç—å –¥–ª—è –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è.")
                    break

            except Exception as e:
                st.warning(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ {name}: {e}")

        st.success(f"üèÅ –ü–æ—à—É–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –û–±—Ä–æ–±–ª–µ–Ω–æ: {num_checked} –∫–æ–º–ø–∞–Ω—ñ–π.")

    except Exception as e:
        st.error(f"‚ùå –ó–∞–≥–∞–ª—å–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")

        st.success(f"üèÅ –ü–æ—à—É–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –û–±—Ä–æ–±–ª–µ–Ω–æ: {num_checked} –∫–æ–º–ø–∞–Ω—ñ–π.")

    except Exception as e:
        st.error(f"‚ùå –ó–∞–≥–∞–ª—å–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
