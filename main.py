import streamlit as st
import requests
import re
from urllib.parse import urlparse
import json
from bs4 import BeautifulSoup
import openai
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# –°–µ–∫—Ä–µ—Ç–∏
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
GSHEET_JSON = st.secrets["GSHEET_SERVICE_ACCOUNT"]
GSHEET_SPREADSHEET_ID = "1S0nkJYXrVTsMHmeOC-uvMWnrw_yQi5z8NzRsJEcBjc0"

# –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Google Sheets
def get_gsheet_client():
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    creds_dict = json.loads(GSHEET_JSON)
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    return gspread.authorize(creds)

# –°–ø—Ä–æ—â–µ–Ω–Ω—è URL
def simplify_url(link):
    parsed = urlparse(link)
    return f"{parsed.scheme}://{parsed.netloc}"

# –Ü–Ω—Ç–µ—Ä—Ñ–µ–π—Å
st.set_page_config(page_title="–ü–æ—à—É–∫ —Å–∞–π—Ç—ñ–≤", layout="wide")
st.title("üîç –ü–æ—à—É–∫ —Å–∞–π—Ç—ñ–≤ –±–µ–∑ –∞–Ω–∞–ª—ñ–∑—É (—Ç—ñ–ª—å–∫–∏ –≤–∫–ª–∞–¥–∫–∞ '–ü–æ—à—É–∫–∏')")

query = st.text_input("–í–≤–µ–¥–∏ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞:")
col1, col2 = st.columns(2)
with col1:
    num_results = st.slider("–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤", min_value=1, max_value=100, value=10, step=1)
with col2:
    start_index = st.number_input("–ü–æ—á–∏–Ω–∞—Ç–∏ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É ‚Ññ", min_value=1, max_value=91, value=1, step=10)

start = st.button("–ü–æ—à—É–∫")

if start and query:
    st.markdown("üîÅ **–¢—Ä–∏–≥–µ—Ä –∞–∫—Ç–∏–≤–æ–≤–∞–Ω–æ ‚Äî –≤–∏–∫–æ–Ω—É—î—Ç—å—Å—è –ø–æ—à—É–∫...**")  # DEBUG

    with st.spinner("–ü–æ—à—É–∫ —Å–∞–π—Ç—ñ–≤..."):
        params = {
            "key": st.secrets["GOOGLE_API_KEY"],
            "cx": st.secrets["CSE_ID"],
            "q": query,
            "num": num_results,
            "start": start_index
        }
        results = requests.get("https://www.googleapis.com/customsearch/v1", params=params).json().get("items", [])

        page_number = ((start_index - 1) // num_results) + 1

        st.markdown(f"### üîç –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è: **{query}**")
        st.markdown(f"‚û°Ô∏è **–°—Ç–æ—Ä—ñ–Ω–∫–∞ ‚Ññ{page_number}** (start_index = `{start_index}`, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –æ—Ç—Ä–∏–º–∞–Ω–æ: `{len(results)}`)")

        gc = get_gsheet_client()
        sh = gc.open_by_key(GSHEET_SPREADSHEET_ID)

        try:
            search_sheet = sh.worksheet("–ü–æ—à—É–∫–∏")
        except:
            search_sheet = sh.add_worksheet(title="–ü–æ—à—É–∫–∏", rows="1000", cols="5")
            search_sheet.append_row(["–ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞", "–ù–∞–∑–≤–∞", "–°–∞–π—Ç", "–°—Ç–æ—Ä—ñ–Ω–∫–∞", "–î–∞—Ç–∞"], value_input_option="USER_ENTERED")

        existing_links = set(search_sheet.col_values(3))
        new_count = 0

        for item in results:
            title = item.get("title", "")
            raw_link = item.get("link", "")
            simplified = simplify_url(raw_link)

            if simplified in existing_links:
                st.markdown(f"üîÅ –ü—Ä–æ–ø—É—â–µ–Ω–æ (–≤–∂–µ —î): `{simplified}`")
                continue

            st.markdown(f"‚úÖ –î–æ–¥–∞–Ω–æ: **{title}** ‚Äî `{simplified}`")

            search_sheet.append_row(
                [query, title, simplified, page_number, st.session_state.get("current_date", "")],
                value_input_option="USER_ENTERED"
            )
            existing_links.add(simplified)
            new_count += 1

        st.success(f"üü¢ –î–æ–¥–∞–Ω–æ {new_count} –Ω–æ–≤–∏—Ö —Å–∞–π—Ç—ñ–≤ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {page_number}")



        
        # --------------------- GPT-–ê–Ω–∞–ª—ñ–∑ –Ω–æ–≤–∏—Ö —Å–∞–π—Ç—ñ–≤ ---------------------
client = openai.OpenAI(api_key=OPENAI_API_KEY)

st.header("ü§ñ GPT-–ê–Ω–∞–ª—ñ–∑ –Ω–æ–≤–∏—Ö —Å–∞–π—Ç—ñ–≤")

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É —Ç–µ–∫—Å—Ç—É —Å–∞–π—Ç—É
def get_page_text(url):
    try:
        response = requests.get(url, timeout=7)
        soup = BeautifulSoup(response.text, "html.parser")
        for tag in soup(["script", "style", "noscript"]):
            tag.extract()
        text = soup.get_text(separator=" ")
        return ' '.join(text.split())[:2000]  # –æ–±–º–µ–∂–µ–Ω–Ω—è –¥–æ 2000 —Å–∏–º–≤–æ–ª—ñ–≤
    except Exception as e:
        return f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ —Ç–µ–∫—Å—Ç —Å–∞–π—Ç—É: {e}"

# –°–∫—ñ–ª—å–∫–∏ —Å–∞–π—Ç—ñ–≤ –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∑–∞ —Ä–∞–∑
num_to_analyze = st.slider("–°–∫—ñ–ª—å–∫–∏ –∑–∞–ø–∏—Å—ñ–≤ –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∑–∞ —Ä–∞–∑", min_value=1, max_value=50, value=10)

if st.button("–ê–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ –∑–∞–ø–∏—Å–∏ GPT"):
    with st.spinner("–ü—Ä–æ–≤–æ–¥–∏—Ç—å—Å—è GPT-–∞–Ω–∞–ª—ñ–∑..."):
        gc = get_gsheet_client()
        sh = gc.open_by_key(GSHEET_SPREADSHEET_ID)

        try:
            search_sheet = sh.worksheet("–ü–æ—à—É–∫–∏")
        except:
            st.error("–í–∫–ª–∞–¥–∫–∞ '–ü–æ—à—É–∫–∏' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞.")
            st.stop()

        try:
            analysis_sheet = sh.worksheet("–ê–Ω–∞–ª—ñ–∑")
        except:
            analysis_sheet = sh.add_worksheet(title="–ê–Ω–∞–ª—ñ–∑", rows="1000", cols="8")
            analysis_sheet.append_row(
                ["–ù–∞–∑–≤–∞", "–°–∞–π—Ç", "–ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞", "–í–∏—Å–Ω–æ–≤–æ–∫", "–ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π –∫–ª—ñ—î–Ω—Ç", "–°—Ç–æ—Ä—ñ–Ω–∫–∞", "–î–∞—Ç–∞", "–°—Ç–∞—Ç—É—Å GPT"],
                value_input_option="USER_ENTERED"
            )

        records = search_sheet.get_all_records()
        analyzed_sites = set()
        existing_analysis = analysis_sheet.get_all_records()
        for r in existing_analysis:
            site_val = r.get("–°–∞–π—Ç", "").strip().lower()
            if site_val:
                analyzed_sites.add(site_val)

        analyzed_count = 0
        for idx, row in enumerate(records, start=2):
            if analyzed_count >= num_to_analyze:
                break

            title = row.get("–ù–∞–∑–≤–∞", "")
            site = simplify_url(row.get("–°–∞–π—Ç", "").strip().lower())
            keywords = row.get("–ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞", "")
            page = row.get("–°—Ç–æ—Ä—ñ–Ω–∫–∞", "")
            date = row.get("–î–∞—Ç–∞", "")

            if site in analyzed_sites:
                continue

            try:
                site_text = get_page_text(site)

                prompt = f"""
                –¢–∏ ‚Äî –∞—Å–∏—Å—Ç–µ–Ω—Ç –∑ –ø—Ä–æ–¥–∞–∂—É –∫–æ–º–ø–∞–Ω—ñ—ó, —è–∫–∞ –ø–æ—Å—Ç–∞—á–∞—î —Ä–µ–Ω—Ç–≥–µ–Ω-–ø–ª—ñ–≤–∫—É, –∫–∞—Å–µ—Ç–∏, –º–µ–¥–∏—á–Ω—ñ –ø—Ä–∏–Ω—Ç–µ—Ä–∏ —Ç–∞ –≤–∏—Ç—Ä–∞—Ç–Ω—ñ –º–∞—Ç–µ—Ä—ñ–∞–ª–∏.

                –ù–∞–∑–≤–∞ –∫–æ–º–ø–∞–Ω—ñ—ó: {title}
                –°–∞–π—Ç: {site}
                –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞: {keywords}
                –ö–æ–Ω—Ç–µ–Ω—Ç —Å–∞–π—Ç—É (–æ–±–º–µ–∂–µ–Ω–æ): {site_text}

                –ó–∞–≤–¥–∞–Ω–Ω—è:
                - –í–∏–∑–Ω–∞—á–∏, —á–∏ –∫–æ–º–ø–∞–Ω—ñ—è —î –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–º –∫–ª—ñ—î–Ω—Ç–æ–º (–¢–∞–∫/–ù—ñ).
                - –Ø–∫—â–æ –¢–∞–∫, –≤–∫–∞–∂–∏ —ó—ó —Ç–∏–ø (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥: –¥–∏—Å—Ç—Ä–∏–±‚Äô—é—Ç–æ—Ä, –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫).
                - –î–∞–π –∫–æ—Ä–æ—Ç–∫–∏–π –≤–∏—Å–Ω–æ–≤–æ–∫ ‚Äî –æ–¥–Ω–µ —Ä–µ—á–µ–Ω–Ω—è, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥: "–¢–∞–∫, –¥–∏—Å—Ç—Ä–∏–±‚Äô—é—Ç–æ—Ä, –ø—Ä–∞—Ü—é—î –∑ –≤–∞—à–∏–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏."

                –§–æ—Ä–º–∞—Ç:
                –ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π –∫–ª—ñ—î–Ω—Ç: –¢–∞–∫/–ù—ñ
                –í–∏—Å–Ω–æ–≤–æ–∫: (–æ–¥–Ω–µ —Ä–µ—á–µ–Ω–Ω—è)
                """

                response = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": prompt}]
                )

                content = response.choices[0].message.content.strip()

                try:
                    client_match = re.search(r"–ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π –∫–ª—ñ—î–Ω—Ç:\s*(–¢–∞–∫|–ù—ñ)", content)
                    summary_match = re.search(r"–í–∏—Å–Ω–æ–≤–æ–∫:\s*(.+)", content)

                    is_client = client_match.group(1) if client_match else "-"
                    summary = summary_match.group(1).strip() if summary_match else content
                except Exception as parse_error:
                    is_client = "-"
                    summary = f"GPT format error: {parse_error}"

                status = "–ê–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ"

            except Exception as e:
                is_client = "-"
                summary = f"–ü–æ–º–∏–ª–∫–∞: {e}"
                status = "–ü–æ–º–∏–ª–∫–∞"

            # –î–æ–¥–∞—î–º–æ –¥–æ "–ê–Ω–∞–ª—ñ–∑" —á—ñ—Ç–∫–æ –≤ 8 –∫–æ–ª–æ–Ω–æ–∫
            analysis_row = [
                title or "-",     # –ù–∞–∑–≤–∞
                site or "-",      # –°–∞–π—Ç
                keywords or "-",  # –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞
                summary or "-",   # –í–∏—Å–Ω–æ–≤–æ–∫
                is_client or "-", # –ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π –∫–ª—ñ—î–Ω—Ç
                page or "-",      # –°—Ç–æ—Ä—ñ–Ω–∫–∞
                date or "-",      # –î–∞—Ç–∞
                status or "-"     # –°—Ç–∞—Ç—É—Å GPT
            ]
            analysis_sheet.append_row(analysis_row, value_input_option="USER_ENTERED")
                        # –ü—ñ—Å–ª—è –∑–∞–ø–∏—Å—É –≤ "–ê–Ω–∞–ª—ñ–∑" –æ–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç—É—Å –≤ "–ü–æ—à—É–∫–∏"
            try:
                search_sheet.update_cell(idx, 7, status)
            except Exception as update_error:
                st.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ–Ω–æ–≤–∏—Ç–∏ —Å—Ç–∞—Ç—É—Å –¥–ª—è '{title}': {update_error}")


            analyzed_sites.add(site)
            analyzed_count += 1

        st.success(f"‚úÖ GPT-–∞–Ω–∞–ª—ñ–∑ –≤–∏–∫–æ–Ω–∞–Ω–æ –¥–ª—è {analyzed_count} –Ω–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤.")
