
import streamlit as st
import requests
import openai
import pandas as pd
import re
from urllib.parse import urlparse
import os

# üîê –°–µ–∫—Ä–µ—Ç–∏ –∑—ñ Streamlit Cloud
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
CSE_ID = st.secrets["CSE_ID"]

client = openai.OpenAI(api_key=OPENAI_API_KEY)

def simplify_url(link):
    parsed = urlparse(link)
    return f"{parsed.scheme}://{parsed.netloc}"

def extract_email(text):
    emails = re.findall(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", text)
    return emails[0] if emails else "‚Äî"

def analyze_with_gpt(title, snippet, link):
    prompt = f"""
    –¢–∏ –¥–æ–ø–æ–º–∞–≥–∞—î—à –≤–∏–∑–Ω–∞—á–∏—Ç–∏, —á–∏ —Å–∞–π—Ç –º–æ–∂–µ –±—É—Ç–∏ –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–º –∫–ª—ñ—î–Ω—Ç–æ–º –¥–ª—è –º–µ–¥–∏—á–Ω–æ—ó –ø—Ä–æ–¥—É–∫—Ü—ñ—ó (Agfa, Fujifilm, Carestream).

    üîπ –ù–∞–∑–≤–∞: {title}
    üîπ –û–ø–∏—Å: {snippet}
    üîπ –õ—ñ–Ω–∫: {link}

    1Ô∏è‚É£ –ß–∏ –º–æ–∂–µ —Ü–µ –±—É—Ç–∏ –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π –∫–ª—ñ—î–Ω—Ç? (–¢–∞–∫ / –ù—ñ + –∫–æ—Ä–æ—Ç–∫–µ –ø–æ—è—Å–Ω–µ–Ω–Ω—è)

    2Ô∏è‚É£ –Ø–∫–∏–π —Ç–∏–ø –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—ó —Ü–µ –π–º–æ–≤—ñ—Ä–Ω–æ?
    (–í–∏–±–µ—Ä–∏ –æ–¥–∏–Ω: –¥–∏—Å—Ç—Ä–∏–±‚Äô—é—Ç–æ—Ä / –ª—ñ–∫–∞—Ä–Ω—è / –º–µ–¥–∏—á–Ω–∏–π —Ü–µ–Ω—Ç—Ä / –≤–∏—Ä–æ–±–Ω–∏–∫ / —ñ–Ω—à–µ)

    –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π —É —Ñ–æ—Ä–º–∞—Ç—ñ:
    –ö–ª—ñ—î–Ω—Ç: –¢–∞–∫/–ù—ñ ‚Äî [–ø–æ—è—Å–Ω–µ–Ω–Ω—è]
    –¢–∏–ø: [—Ç–∏–ø –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—ó]
    """
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

st.set_page_config(page_title="–ü–æ—à—É–∫ –∫–ª—ñ—î–Ω—Ç—ñ–≤ GPT", layout="wide")
st.title("üîç –ü–æ—à—É–∫ –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏—Ö –∫–ª—ñ—î–Ω—Ç—ñ–≤ —á–µ—Ä–µ–∑ Google + GPT")

query = st.text_input("–í–≤–µ–¥–∏ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞:")

col1, col2 = st.columns(2)
with col1:
    num_results = st.slider("–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤", min_value=1, max_value=100, value=10, step=1)
with col2:
    start_index = st.number_input("–ü–æ—á–∏–Ω–∞—Ç–∏ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É ‚Ññ", min_value=1, max_value=91, value=1, step=10)

filter_yes_only = st.checkbox("–ü–æ–∫–∞–∑–∞—Ç–∏ –ª–∏—à–µ '–ö–ª—ñ—î–Ω—Ç: –¢–∞–∫'")
start = st.button("–ü–æ—à—É–∫")

if start and query:
    cache_filename = f"results_{query.replace(' ', '_').lower()}.csv"

    # –Ø–∫—â–æ —Ñ–∞–π–ª —ñ—Å–Ω—É—î ‚Äî –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –π–æ–≥–æ
    if os.path.exists(cache_filename):
        existing_df = pd.read_csv(cache_filename, sep=";", encoding="utf-8-sig")
        existing_urls = set(existing_df["–î–æ–º–∞—à–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∞"])
    else:
        existing_df = pd.DataFrame()
        existing_urls = set()

    with st.spinner("–ü–æ—à—É–∫ —Ç–∞ GPT-–∞–Ω–∞–ª—ñ–∑..."):
        params = {
            "key": GOOGLE_API_KEY,
            "cx": CSE_ID,
            "q": query,
            "num": num_results,
            "start": start_index
        }
        results = requests.get("https://www.googleapis.com/customsearch/v1", params=params).json().get("items", [])
        all_data = []

        for item in results:
            title = item["title"]
            raw_link = item["link"]
            link = simplify_url(raw_link)
            snippet = item.get("snippet", "")
            email = extract_email(title + " " + snippet)

            if link in existing_urls:
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –≤–∂–µ –æ–±—Ä–æ–±–ª–µ–Ω—ñ

            try:
                gpt_response = analyze_with_gpt(title, snippet, link)
                client_result, org_type = gpt_response.split("–¢–∏–ø:", 1)
                client_result = client_result.strip().replace("–ö–ª—ñ—î–Ω—Ç:", "").strip()
                org_type = org_type.strip()
            except Exception as e:
                client_result = f"–ü–æ–º–∏–ª–∫–∞: {e}"
                org_type = "–ù–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ"

            all_data.append({
                "–ù–∞–∑–≤–∞": title,
                "–î–æ–º–∞—à–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∞": link,
                "–ü–æ—à—Ç–∞": email,
                "–¢–∏–ø": org_type,
                "GPT-–≤–∏—Å–Ω–æ–≤–æ–∫": client_result,
                "–û–ø–∏—Å": snippet
            })

        new_df = pd.DataFrame(all_data)
        combined_df = pd.concat([existing_df, new_df], ignore_index=True).drop_duplicates(subset=["–î–æ–º–∞—à–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∞"])

        if filter_yes_only:
            combined_df = combined_df[combined_df["GPT-–≤–∏—Å–Ω–æ–≤–æ–∫"].str.startswith("–¢–∞–∫")]

        if combined_df.empty:
            st.info("–ù–µ–º–∞—î –Ω–æ–≤–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∞–±–æ –Ω—ñ—á–æ–≥–æ –Ω–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –∑–∞ —Ñ—ñ–ª—å—Ç—Ä–æ–º.")
        else:
            st.success("–ì–æ—Ç–æ–≤–æ!")
            for i in range(len(combined_df)):
                with st.expander(f"üîó {combined_df.iloc[i]['–ù–∞–∑–≤–∞']}"):
                    st.markdown(f"**–î–æ–º–∞—à–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∞:** [{combined_df.iloc[i]['–î–æ–º–∞—à–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∞']}]({combined_df.iloc[i]['–î–æ–º–∞—à–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∞']})")
                    st.markdown(f"**–ü–æ—à—Ç–∞:** {combined_df.iloc[i]['–ü–æ—à—Ç–∞']}")
                    st.markdown(f"**–¢–∏–ø:** {combined_df.iloc[i]['–¢–∏–ø']}")
                    st.markdown(f"**GPT-–≤–∏—Å–Ω–æ–≤–æ–∫:** {combined_df.iloc[i]['GPT-–≤–∏—Å–Ω–æ–≤–æ–∫']}")
                    st.markdown(f"**–û–ø–∏—Å:** {combined_df.iloc[i]['–û–ø–∏—Å']}")
                    st.markdown("---")

            csv_data = combined_df.to_csv(index=False, sep=";", encoding="utf-8-sig")
            st.download_button("‚¨áÔ∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ CSV", data=csv_data, file_name=cache_filename, mime="text/csv")
            combined_df.to_csv(cache_filename, index=False, sep=";", encoding="utf-8-sig")
